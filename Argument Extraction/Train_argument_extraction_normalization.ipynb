{"cells":[{"cell_type":"markdown","metadata":{"id":"aHFEf8hG4RUB"},"source":["# Triple and Perspective Extraction and Scoring with Normalization\n","\n","In this notebook you will be fine-tuning a language model to perform triple argument (Subject, Predicate, Object) extraction and candidate triple scoring. For the predicates, you will create various categories, and the aim of the model is to find the predicate token span as well as the most likely category for the predicate. You will use the `pytorch` implementation of `albert-base` provided by the Huggingface `transformers` library and fine-tune this model on PersonaChat, DailyDialog and Circa data annotated with ground-truth triples. You will also try adapt the code to allow for other models and train those models to compare model performances on the normalized predicates. \n","\n","## Overview \n","Adopting a two-stage setup allows maximum flexibility of the triple extraction while making efficient use of the annotated data. The two stages include:\n","\n","1. A sequence labeling (BIO-tagging) model which extracts lists of subjects, predicates and objects from the input dialogue.\n","\n","2. A model which takes combinations of subjects, predicates and objects found and scores these combinations (i.e. all candidate triples) to decide whether the triple can indeed be entailed from the dialogue and what its polarity is.\n","\n","By using this two-stage approach, arbitrary numbers of triples can be extracted and linguistic phenomena such as ellipsis can be accounted for.<br>\n","\n","The first part is most relevant for the extraction of abstract predicates, whereas the second part is relevant for the evaluation."]},{"cell_type":"markdown","metadata":{"id":"u6XnlD6P5Tl2"},"source":["## Getting the Data\n","\n","To get a dataset of ground truth triples a small development set was created. This data has been stored in Google Drive for easy access."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26714,"status":"ok","timestamp":1671284191603,"user":{"displayName":"Dorien Renting","userId":"02236066021659360815"},"user_tz":-60},"id":"n9SAUVRc4Jtc","outputId":"efd5249a-61df-44f4-c7ff-58b6a615b7a2"},"outputs":[],"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","root_dir = '/content/gdrive/MyDrive/Communicative Robotics' \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28556,"status":"ok","timestamp":1671284232944,"user":{"displayName":"Dorien Renting","userId":"02236066021659360815"},"user_tz":-60},"id":"elKejMJz6tfG","outputId":"38217d72-5afb-46ab-a252-61fe1d8fb358"},"outputs":[],"source":["import glob\n","import json\n","import random\n","\n","def load_annotations(path, remove_unk=True, keep_skipped=False):\n","    \"\"\" Reads all annotation files from path. By default, it filters skipped\n","        files and removes the [unk] tokens appended at the end of each turn.\n","\n","        params:\n","        str path:           name of directory containing annotations\n","        bool remove_unk:    whether to remove [unk] tokens (default: True)\n","        bool keep_skipped:  whether to keep skipped annotations (default: False)\n","\n","        returns:    list of annotations dicts\n","    \"\"\"\n","    annotations = []\n","    for fname in glob.glob(path + '/*.json'):\n","        with open(fname, 'r', encoding='utf-8') as file:\n","            data = json.load(file)\n","\n","            if data['skipped'] and not keep_skipped:\n","                continue\n","\n","            if remove_unk:\n","                data['tokens'] = [[t for t in turn if t != '[unk]'] for turn in data['tokens']]\n","\n","            annotations.append(data)\n","\n","    return annotations\n","\n","annotations = load_annotations(root_dir + '/annotated_data/trainval') #include in new dir\n","annotations[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1671284238331,"user":{"displayName":"Dorien Renting","userId":"02236066021659360815"},"user_tz":-60},"id":"qXwUWdkOEcmo","outputId":"d0a7ab3f-4694-43ca-9540-3d5dc113ee36"},"outputs":[],"source":["print('#dialogs:', len(annotations))\n","print('#triples:', sum([sum([any(t) for t in d['annotations']]) for d in annotations]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mikx1fcmENVY"},"outputs":[],"source":["def get_predicate_tokens(annotation, triple):\n","    # if triple[1]:\n","    #     turn = triple[1][0][0] \n","    #     start = triple[1][0][1]\n","    #     end = triple[1][-1][1] #volgens mij klopt triple[1][-1][0] niet, moet triple[1][-1][1] zijn denk ik\n","    #     return ' '.join(annotation['tokens'][turn][start:end + 1])\n","    # else:\n","    #     return None\n","\n","\n","    # heb deze functie verandert zodat ie niet alle tokens van start tot end pakt (dan pakt hij namelijk ook wel eens een subject of object token)\n","    # nu returnt ie alleen de tokens die geannoteerd zijn als predicate (zitten soms nog steeds subject/object bij, maar minder dan eerst)\n","      if triple[1]:\n","        turn = triple[1][0][0]\n","        pred_tokens = []\n","        for n in range(0, len(triple[1])):\n","          token_index = triple[1][n][1]\n","          token = annotation['tokens'][turn][token_index]\n","          pred_tokens.append(token)\n","        return ' '.join(pred_tokens)\n","      else:\n","        return None\n","\n","\n","# triple[1] verwijst naar tweede object in triple --> predicate. \n","# triple[1][0][0] verwijst naar eerste integer (conversation turn) in de eerste token van dit predicate\n","# triple[1][0][1] verwijst naar tweede integer (token) in de eerste token van dit predicate\n","# triple [1][-1][0] verwijst naar eerste integer (conversation turn) in de laatste token van dit predicate --> niet logisch want we willen de index van de laatste token niet de turn\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ofy6UggYUMy"},"outputs":[],"source":["# for ann in annotations:\n","#   for triple in ann['annotations']:\n","#     print('predictate is: ',get_predicate_tokens(ann, triple))"]},{"cell_type":"markdown","metadata":{"id":"CvSlrU9dH2xp"},"source":["## Who are 'You'?: Disambiguating You and I\n","\n","In the text the speakers are referred to as ambiguous tokens *You* and *I*. As these words are ambiguous and their meaning depends on the speaker who utters them, we replace these tokens by [SPEAKER1] and [SPEAKER2] contingent on the speaker (e.g. speaker 2 saying you indicates, [speaker1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlETfhBLIDeE"},"outputs":[],"source":["SPEAKER1 = 'SPEAKER1'\n","SPEAKER2 = 'SPEAKER2'\n","\n","def disambiguate_pronouns(token, turn_idx):\n","    # Even turns -> speaker1\n","    if turn_idx % 2 == 0:\n","        if token in ['i', 'me', 'myself', 'we', 'ourselves']:\n","            return SPEAKER1\n","        elif token in ['my', 'mine', 'our', 'ours']:\n","            return SPEAKER1 + \"'s\"\n","        elif token in ['you', 'yourself', 'yourselves']:\n","            return SPEAKER2\n","        elif token in ['your', 'yours']:\n","            return SPEAKER2 + \"'s\"\n","    else:\n","        if token in ['i', 'me', 'myself', 'we', 'ourselves']:\n","            return SPEAKER2\n","        elif token in ['my', 'mine', 'our', 'ours']:\n","            return SPEAKER2 + \"'s\"\n","        elif token in ['you', 'yourself', 'yourselves']:\n","            return SPEAKER1\n","        elif token in ['your', 'yours']:\n","            return SPEAKER1 + \"'s\"\n","    return token"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671284251875,"user":{"displayName":"Dorien Renting","userId":"02236066021659360815"},"user_tz":-60},"id":"Y0XasWY9IGNm","outputId":"ff5eea0a-01d6-43d6-9b08-2b4d42a563b4"},"outputs":[],"source":["for annotation in annotations:\n","    annotation['tokens'] = [[disambiguate_pronouns(token, i % 2) for token in turn] for i, turn in enumerate(annotation['tokens'])]\n","annotations[0]"]},{"cell_type":"markdown","metadata":{"id":"sNHyj5A8x6a6"},"source":["# Creating sets of abstract predicates\n","\n","First, we create a set consisting of all unique token spans annotated as 'predicate'. This is the set from which you need to create a set of abstract predicates. Notice that a lot of 'unique predicates' are also annotation errors, where the subject or object has been included in the predicate annotation. \n","\n","For each of the abstract predicates you define, you will need to create a numerical B- and I-tag for the BIO tag annotation that we will use. Start from (3,4) for the first tag (e.g. B-like, I-like). The tag 0 is reserved for 'O', and 1 and 2 are reserved for the B- and I-tags for subjects and objects. \n","\n","O-subject: 0   \n","B-subject: 1   \n","I-subject: 2   \n","\n","O-object: 0   \n","B-object: 1   \n","I-object: 2    \n","\n","O-predicate: 0   \n","\n","I-predicate1: 3   \n","B-predicate1: 4   \n","I-predicate2: 5   \n","B-predicate2: 6   \n","I-predicate3: 7   \n","B-predicate3: 8   \n","I-predicate4: 9   \n","B-predicate4: 10   \n","etc\n","We will define these predicates\n","\n","Once you've created a set of abstract predicates and their corresponding B- and I-tags, you will need to create two lookup dictionaries for the BIO tagging that we will do later on. The dictionary `lookup` is used to get the correct BIO tag from the predicate token span and is used for converting triples to BIO-tags. The dictionary `bio_lookup` is used to get the abstract predicate from the BIO tag, and is used to convert BIO tags to tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3iaMqomRRgGX"},"outputs":[],"source":["unique_predicates = set()\n","for ann in annotations:\n","  for triple in ann['annotations']:\n","    pred = get_predicate_tokens(ann, triple)\n","    if pred is not None:\n","      unique_predicates.add(pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":196,"status":"ok","timestamp":1671189735188,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"CbnMK66kRiOZ","outputId":"2a7a1609-e3a1-4baf-beec-19477c879335"},"outputs":[],"source":["print(unique_predicates)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kOrkBzN7tPk9"},"source":["## Read train dictionarys from json file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTNT0UlhrEHK"},"outputs":[],"source":["with open(root_dir+'/conversion_dict_level1.json', 'r', encoding='utf-8') as fin:\n","  train_dict_level1 = json.load(fin)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lglGCqGFrtGZ"},"outputs":[],"source":["with open(root_dir+'/conversion_dict_level2.json', 'r', encoding='utf-8') as fin:\n","  train_dict_level2 = json.load(fin)"]},{"cell_type":"markdown","metadata":{"id":"zVRfMv_CqqG2"},"source":["## Continue with bio tagging"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":406,"status":"ok","timestamp":1671284270609,"user":{"displayName":"Dorien Renting","userId":"02236066021659360815"},"user_tz":-60},"id":"lw9tkGC5CAM8","outputId":"f66f7548-cad6-431a-b396-43b1b7072cd3"},"outputs":[],"source":["## annotatie van training data met BIO tags\n","## Inlezen van json file en mappen naar abstracte predicaten\n","\n","## Biotag level 1\n","\n","counter = 3\n","bio_lookup_l1 = {}\n","bio_dict_l1 = {}\n","lookup_l1 = {}\n","\n","for abstract, predicates in train_dict_level1.items():\n","  bio_lookup_l1[counter] = abstract\n","  bio_dict_l1[(counter,counter+1)] = predicates\n","  counter += 2\n","\n","for key, value in bio_dict_l1.items():\n","    for pred in value:\n","        lookup_l1[pred] = key\n","\n","print(lookup_l1)\n","print(bio_lookup_l1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":550,"status":"ok","timestamp":1671284273732,"user":{"displayName":"Dorien Renting","userId":"02236066021659360815"},"user_tz":-60},"id":"shLTJyRrjLHM","outputId":"042e0af0-2c24-451e-840d-055f121c3145"},"outputs":[],"source":["## annotatie van training data met BIO tags\n","## Inlezen van json file en mappen naar abstracte predicaten\n","\n","## Biotag level 2\n","\n","counter = 3\n","bio_lookup_l2 = {}\n","bio_dict_l2 = {}\n","lookup_l2 = {}\n","\n","for abstract, predicates in train_dict_level2.items():\n","  bio_lookup_l2[counter] = abstract\n","  bio_dict_l2[(counter,counter+1)] = predicates\n","  counter += 2\n","\n","for key, value in bio_dict_l2.items():\n","    for pred in value:\n","        lookup_l2[pred] = key\n","\n","print(lookup_l2)\n","print(bio_lookup_l2)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"T8Uk-jO-bi7T"},"source":["## Abstract predicates Evaluation data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IIbsrW-0bhcA"},"outputs":[],"source":["# Verandert de predicate in de evaluatie data naar een abstract predicaat\n","\n","def load_evaluation_data(path):\n","  \"\"\"\n","  Read an file as txt file\n","\n","  Returns \n","  \"\"\"\n","  with open(path, 'r', encoding='utf-8') as file:\n","    data = file.readlines()\n","    return data\n","\n","\n","def abstract_predicates(file, abstract_dict, lookup, bio_lookup, root_dir):\n","  dict_values = []\n","  abstract_content = \"\"\n","  for values in abstract_dict.values():\n","    for value in values:\n","      dict_values.append(value)\n","\n","  # filename = root_dir + '/annotated_data/evaluatiedata/test_declarative_statements_abstract_val_l1.txt'\n","  # filename = root_dir + '/annotated_data/evaluatiedata/test_single_utterances_abstract_val_l1.txt'\n","  # filename = root_dir + '/annotated_data/evaluatiedata/test_coreference_abstract_val_l1.txt'\n","\n","  for row in file:\n","    if len(row.split(\",\")) != 4:\n","      append_abstract_content(filename, row)\n","    else:\n","      triple = row.split(\",\")\n","      predicate = triple[1]\n","      if predicate in dict_values:\n","        key = lookup[predicate]\n","        abstract_pred = bio_lookup[key[0]]\n","        row = row.replace(predicate, abstract_pred)\n","      append_abstract_content(filename, row)\n","  return abstract_content\n","\n","def append_abstract_content(filename, content):\n","  with open(filename, \"a\") as outfile:\n","    outfile.write(content)\n","\n","# file = load_evaluation_data(root_dir + '/annotated_data/evaluatiedata/test_declarative_statements_val.txt')\n","# file = load_evaluation_data(root_dir + '/annotated_data/evaluatiedata/test_single_utterances_val.txt')\n","# file = load_evaluation_data(root_dir + '/annotated_data/evaluatiedata/test_coreference_val.txt')\n","\n","# abstract_pedicate = abstract_predicates(file, bio_dict_l1, lookup_l1, bio_lookup_l1, root_dir)\n","# abstract_pedicate = abstract_predicates(file, bio_dict_l2, lookup_l2, bio_lookup_l2, root_dir)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MMyhZF4e71Yl"},"source":["## Converting formats\n","\n","Triple arguments are stored as lists of indices (e.g. [[0, 1], [0, 2]] indicating the second and third token of the first turn). We rather use a BIO tagging scheme to indicate these arguments as a vector of labels (one label for each token in the dialog).\n","\n","Moreover, we flatten the dialogue turns into one flat dialogue using `<eos>` as a separator token."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFJv2Avc70hz"},"outputs":[],"source":["from numpy.lib.function_base import kaiser\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","\n","def triple_to_bio_tags(annotation, arg, lookup):\n","    \"\"\" Converts the token indices of the annotations to a vector of BIO labels\n","        for an argument.\n","\n","        params:\n","        dict annotation:    loaded annotation file (see load_annotations)\n","        int arg:            argument to create tag sequence for (subj=0, pred=1, obj=2)\n","\n","        returns:    ndarray with BIO labels (I=2, B=1, O=0)\n","    \"\"\" \n","    # Determine length of dialogue\n","    turns = annotation['tokens']\n","    triples = annotation['annotations']\n","    num_tokens = sum([len(turn) + 1 for turn in turns])  # +1 for <eos>\n","\n","    # Create vector same size as dialogue\n","    mask = np.zeros(num_tokens, dtype=np.uint8)\n","\n","    # Label annotated arguments as BIO tags\n","    for triple in triples:\n","        if arg == 1:\n","            pred = get_predicate_tokens(annotation, triple)\n","            if pred is not None:\n","                try:\n","                  pred = pred.strip()\n","                  pred = pred.strip(\"'\")\n","                  pred = pred.strip()\n","                  B_tag, I_tag = lookup[pred]\n","                except:\n","                  B_tag, I_tag = (287,288)\n","\n","                for j, (turn_id, token_id) in enumerate(triple[arg]):\n","                    k = sum([len(t) + 1 for t in turns[:turn_id]]) + token_id  # k = index of token in dialogue\n","                    if j == 0:\n","                      mask[k] = B_tag\n","                    else:\n","                      mask[k] = I_tag\n","        else:\n","            for j, (turn_id, token_id) in enumerate(triple[arg]):\n","                k = sum([len(t) + 1 for t in turns[:turn_id]]) + token_id  # k = index of token in dialogue\n","                mask[k] = 1 if j == 0 else 2\n","        \n","    return mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMfVIluz8xTA"},"outputs":[],"source":["## LEVEL 1\n","tokens, labels = [], []\n","for ann in annotations:\n","    # Map triple arguments to BIO tagged masks\n","    labels.append((triple_to_bio_tags(ann, 0, lookup_l1),\n","                   triple_to_bio_tags(ann, 1, lookup_l1),\n","                   triple_to_bio_tags(ann, 2, lookup_l1)))\n","    \n","    # Flatten turn sequence\n","    tokens.append([t for ts in ann['tokens'] for t in ts + ['<eos>']])\n","    \n","# Show as BIO scheme\n","i = random.randint(0, len(tokens) - 1)\n","pd.DataFrame(labels[i], columns=tokens[i], index=['subj', 'pred', 'obj'])\n","\n","tokens_l1 = tokens\n","labels_l1 = labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QWSq3ihzjskA"},"outputs":[],"source":["## LEVEL 2\n","tokens, labels = [], []\n","for ann in annotations:\n","    # Map triple arguments to BIO tagged masks\n","    labels.append((triple_to_bio_tags(ann, 0, lookup_l2),\n","                   triple_to_bio_tags(ann, 1, lookup_l2),\n","                   triple_to_bio_tags(ann, 2, lookup_l2)))\n","    \n","    # Flatten turn sequence\n","    tokens.append([t for ts in ann['tokens'] for t in ts + ['<eos>']])\n","    \n","# Show as BIO scheme\n","i = random.randint(0, len(tokens) - 1)\n","pd.DataFrame(labels[i], columns=tokens[i], index=['subj', 'pred', 'obj'])\n","\n","tokens_l2 = tokens\n","labels_l2 = labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9lIqZvzKlDF"},"outputs":[],"source":["import re\n","\n","def bio_tags_to_tokens(tokens, mask, bio_lookup, predicate=False, one_hot=False):\n","    \"\"\" Converts a vector of BIO-tags into spans of tokens. If BIO-tags are one-hot encoded,\n","        one_hot=True will first perform an argmax to obtain the BIO labels.\n","\n","        params:\n","        list tokens:    list of subwords or tokens (as tokenized by Albert/AutoTokenizer)\n","        ndarray mask:   list of bio labels (one for each subword or token in 'tokens')\n","        bool one_hot:   whether to interpret mask as a one-hot encoded sequence of shape |sequence|x3\n","    \"\"\"\n","    out = []\n","    span = []\n","    for i, token in enumerate(tokens):\n","        pred = mask[i]\n","\n","        # Reverse one-hot encoding (optional)\n","        if one_hot:\n","            pred = np.argmax(pred)\n","          \n","\n","        if pred %2 == 1:  # B\n","            if predicate:\n","                span = bio_lookup[pred]\n","                out.append(span)\n","\n","            else:\n","                span = re.sub('[^\\w\\d\\-\\']+', ' ', ''.join(span)).strip()\n","                out.append(span)\n","                span = [token]\n","\n","        elif pred != 0 and pred %2 == 0:  # I\n","            if predicate:\n","                continue\n","            else:\n","                span.append(token)\n","\n","    if span:\n","        span = re.sub('[^\\w\\d\\-\\']+', ' ', ''.join(span)).strip()\n","        out.append(span)\n","\n","    # Remove empty strings and duplicates\n","    return set([span for span in out if span.strip()])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671284303851,"user":{"displayName":"Dorien Renting","userId":"02236066021659360815"},"user_tz":-60},"id":"KvPSuk30DGOs","outputId":"aecf2be5-0d64-4cbc-8801-d931379b203f"},"outputs":[],"source":["## LEVEL 1 \n","\n","## ONLY PRINTS ONE PREDICATE (DOES PRINT MULTIPLE SUBJECTS AND OBJECTS)\n","i = random.randint(0, len(labels_l1))\n","print(' '.join(tokens_l1[i]) + '\\n')\n","\n","print('Subjects:')\n","print(bio_tags_to_tokens(['+' + t for t in tokens_l1[i]], labels_l1[i][0], bio_lookup_l1))\n","\n","print('\\nPredicates:')\n","print(bio_tags_to_tokens(['+' + t for t in tokens_l1[i]], labels_l1[i][1], bio_lookup_l1, predicate=True))\n","\n","print('\\nObjects:')\n","print(bio_tags_to_tokens(['+' + t for t in tokens_l1[i]], labels_l1[i][2], bio_lookup_l1))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1671284309314,"user":{"displayName":"Dorien Renting","userId":"02236066021659360815"},"user_tz":-60},"id":"N2A_A7OF6hrm","outputId":"6077ecd6-ce95-4c4b-af22-b366e65707a4"},"outputs":[],"source":["## LEVEL 2\n","\n","## ONLY PRINTS ONE PREDICATE (DOES PRINT MULTIPLE SUBJECTS AND OBJECTS)\n","i = random.randint(0, len(labels_l2))\n","print(' '.join(tokens_l2[i]) + '\\n')\n","\n","print('Subjects:')\n","print(bio_tags_to_tokens(['+' + t for t in tokens_l2[i]], labels_l2[i][0], bio_lookup_l2))\n","\n","print('\\nPredicates:')\n","print(bio_tags_to_tokens(['+' + t for t in tokens_l2[i]], labels_l2[i][1], bio_lookup_l2, predicate=True))\n","\n","print('\\nObjects:')\n","print(bio_tags_to_tokens(['+' + t for t in tokens_l2[i]], labels_l2[i][2], bio_lookup_l2))"]},{"cell_type":"markdown","metadata":{"id":"zmHDBXSdD58b"},"source":["## Setting up ALBERT for Argument Extraction\n","\n","Now we set up ALBERT with a token classification head for each of the arguments. To this end we will use PyTorch to create a small linear classifier for each argument which we can slide over the output of ALBERT to make a prediction for each token. To train other models, you should adapt this code so that it works with your model of choice."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTzfnWdmEYE9"},"outputs":[],"source":["%%capture \n","!pip install transformers\n","\n","import torch\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from tqdm import tqdm\n","import numpy as np\n","import random\n","from datetime import date"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":409,"status":"ok","timestamp":1671189756415,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"SGoQYPnXJSIG","outputId":"5af52a84-ee5a-42f1-d14a-992bc89c0967"},"outputs":[],"source":["print(bio_dict_l1) #504 + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1671189756417,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"a99BkzgqJcBW","outputId":"0e04624d-aa56-45b3-aaaf-fb1927b03f8c"},"outputs":[],"source":["print(bio_dict_l2) #340 + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RycF3DNjE5zg"},"outputs":[],"source":["class ArgumentExtraction(torch.nn.Module):\n","    def __init__(self, base_model='albert-base-v2', path=None, output_dim=505, sep='<eos>'): # You need to change the output_dim to the total number of BIO-tags (including 0,1,2)\n","        \"\"\" Init model with multi-span extraction heads for SPO arguments.\n","\n","            params:\n","            str base_model: Transformer architecture to use (default: albert-base-v2)\n","            str path:       Path to pretrained model\n","        \"\"\"\n","        super().__init__()\n","        print('loading %s for argument extraction' % base_model)\n","        self._model = AutoModel.from_pretrained(base_model)\n","        self._base = base_model\n","        self._sep = sep\n","\n","        # Load and extend tokenizer with special SPEAKER tokens\n","        self._tokenizer = AutoTokenizer.from_pretrained(base_model)\n","        self._tokenizer.add_tokens(['SPEAKER1', 'SPEAKER2'], special_tokens=True)\n","        self._model.resize_token_embeddings(len(self._tokenizer))\n","\n","        # Add token classification heads\n","        hidden_size = AutoConfig.from_pretrained(base_model).hidden_size\n","        self._subj_head = torch.nn.Linear(hidden_size, output_dim)\n","        self._pred_head = torch.nn.Linear(hidden_size, output_dim)\n","        self._obj_head = torch.nn.Linear(hidden_size, output_dim)\n","        self._output_dim = output_dim\n","\n","        self._relu = torch.nn.ReLU()\n","        self._softmax = torch.nn.Softmax(dim=-1)\n","\n","        # Set GPU if available\n","        self._device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","        self.to(self._device)\n","\n","        # Load model / tokenizer if pretrained model is given\n","        if path:\n","            print('\\t- Loading pretrained')\n","            model_path = path + '/argument_extraction_' + base_model\n","            self.load_state_dict(torch.load(model_path, map_location=self._device))\n","\n","    def forward(self, input_ids, speaker_ids):\n","        \"\"\" Computes BIO label probabilities for each token\n","        \"\"\"\n","        # Feed dialog through transformer\n","        y = self._model(input_ids=input_ids, token_type_ids=speaker_ids)\n","        h = self._relu(y.last_hidden_state)\n","\n","        # Predict spans\n","        y_subj = self._softmax(self._subj_head(h))\n","        y_pred = self._softmax(self._pred_head(h))\n","        y_obj_ = self._softmax(self._obj_head(h))\n","\n","        # Permute output as tensor of shape (N, |C|, seq_len)\n","        y_subj = y_subj.permute(0, 2, 1)\n","        y_pred = y_pred.permute(0, 2, 1)\n","        y_obj_ = y_obj_.permute(0, 2, 1)\n","        return y_subj, y_pred, y_obj_\n","\n","    def _retokenize_tokens(self, tokens):\n","        # Tokenize each token individually (keeping track of subwords)\n","        input_ids = [[self._tokenizer.cls_token_id]]\n","        for t in tokens:\n","            if t != '<eos>':\n","                input_ids.append(self._tokenizer.encode(t, add_special_tokens=False))\n","            else:\n","                input_ids.append([self._tokenizer.eos_token_id])\n","\n","        # Flatten input_ids\n","        f_input_ids = torch.LongTensor([[i for ids in input_ids for i in ids]]).to(self._device)\n","\n","        # Determine how often we need to repeat the labels\n","        repeats = [len(ids) for ids in input_ids]\n","\n","        # Set speaker IDs\n","        speaker_ids = [0] + [tokens[:i + 1].count(self._sep) % 2 for i in range(len(tokens))][:-1]  # TODO: make pretty\n","        speaker_ids = self._repeat_speaker_ids(speaker_ids, repeats)\n","\n","        return f_input_ids, speaker_ids, repeats\n","\n","    def _repeat_speaker_ids(self, speaker_ids, repeats):\n","        \"\"\" Repeats speaker IDs for oov tokens.\n","        \"\"\"\n","        rep_speaker_ids = np.repeat([0] + list(speaker_ids), repeats=repeats)\n","        return torch.LongTensor([rep_speaker_ids]).to(self._device)\n","\n","    def _repeat_labels(self, labels, repeats):\n","        \"\"\" Repeats BIO labels for OOV tokens. Ensure B-labeled tokens are repeated\n","            as B-I-I etc.\n","        \"\"\"\n","        # Repeat each label b the amount of subwords per token\n","        rep_labels = []\n","        for label, rep in zip([0] + list(labels), repeats):\n","            # Outside\n","            if label == 0:\n","                rep_labels += [label] * rep\n","            # Beginning + Inside\n","            elif (label % 2) == 1:\n","                rep_labels += [label] + ([label+1] * (rep - 1))  # If label = B -> because all B-tags are odd numbers\n","            else:\n","               rep_labels += [label] + ([label] * (rep - 1))  # If label = I -> do not add 1, but keep the same \n","        return torch.LongTensor([rep_labels]).to(self._device)\n","\n","    def fit(self, tokens, labels, epochs=2, lr=1e-5, weight=3):\n","        \"\"\" Fits the model to the annotations\n","        \"\"\"\n","        # Re-tokenize to obtain input_ids and associated labels\n","        X = []\n","        for token_seq, (subj_labels, pred_labels, _obj_labels) in zip(tokens, labels):\n","            input_ids, speaker_ids, repeats = self._retokenize_tokens(token_seq)\n","            subj_labels = self._repeat_labels(subj_labels, repeats)  # repeat when split into subwords\n","            pred_labels = self._repeat_labels(pred_labels, repeats)\n","            _obj_labels = self._repeat_labels(_obj_labels, repeats)\n","            X.append((input_ids, speaker_ids, subj_labels, pred_labels, _obj_labels))\n","\n","        # Set up optimizer\n","        optim = torch.optim.Adam(self.parameters(), lr=lr)\n","\n","        # Higher weight for B- and I-tags to account for class imbalance\n","        class_weights = torch.Tensor([1] + [weight] * (self._output_dim - 1)).to(self._device)\n","        criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","        print('Training!')\n","        for epoch in range(epochs):\n","            losses = []\n","            random.shuffle(X)\n","            for input_ids, speaker_ids, subj_y, pred_y, obj_y in tqdm(X):\n","                # Forward pass\n","                subj_y_hat, pred_y_hat, obj_y_hat = self(input_ids, speaker_ids)\n","\n","                # Compute loss\n","                loss = criterion(subj_y_hat, subj_y)\n","                loss += criterion(pred_y_hat, pred_y)\n","                loss += criterion(obj_y_hat, obj_y)\n","                losses.append(loss.item())\n","\n","                optim.zero_grad()\n","                loss.backward()\n","                optim.step()\n","\n","            print(\"mean loss =\", np.mean(losses))\n","\n","        # Save model to file\n","        torch.save(self.state_dict(), 'argument_extraction_%s' % self._base)\n","\n","    def predict(self, token_seq):\n","        \"\"\" Predicts \"\"\"\n","        # Retokenize token sequence\n","        input_ids, speaker_ids, _ = self._retokenize_tokens(token_seq)\n","\n","        # Invert tokenization for viewing\n","        subwords = self._tokenizer.convert_ids_to_tokens(input_ids[0])\n","\n","        # Forward-pass\n","        predictions = self(input_ids, speaker_ids)\n","        subjs = predictions[0].cpu().detach().numpy()[0]\n","        preds = predictions[1].cpu().detach().numpy()[0]\n","        objs = predictions[2].cpu().detach().numpy()[0]\n","\n","        return subjs, preds, objs, subwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F86HlkfLgZKU"},"outputs":[],"source":["def save_model(level, epochs, weight):\n","  import os, shutil\n","  \n","  out_dir = root_dir + '/models/' + level + '_' + str(epochs) + '_' + str(weight) + '_' + str(date.today())\n","  if not os.path.exists(out_dir):\n","      os.mkdir(out_dir)\n","\n","  shutil.copy('argument_extraction_albert-base-v2', out_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270219,"status":"ok","timestamp":1671190814286,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"8YSOXXZ-8aW2","outputId":"ac777b4b-d45a-4f49-a4a4-dfb8f89d99fe"},"outputs":[],"source":["model_l1 = ArgumentExtraction(output_dim = 505)  \n","epochs = 4\n","weight = 60\n","model_l1.fit(tokens_l1, labels_l1, epochs=epochs, weight=weight)\n","save_model('l1',epochs,weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oSEjZnLqIkfR"},"outputs":[],"source":["model_l2 = ArgumentExtraction(output_dim = 341)  \n","epochs = 6\n","weight = 40\n","model_l2.fit(tokens_l2, labels_l2, epochs=epochs, weight=weight)\n","save_model('l2',epochs,weight)"]},{"cell_type":"markdown","metadata":{"id":"VU9wc9r0E8K3"},"source":["## Putting It All Together\n","\n","Below you can see the token assignments with the BIO scheme to SPO arguments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2916,"status":"ok","timestamp":1671285417203,"user":{"displayName":"Dorien Renting","userId":"02236066021659360815"},"user_tz":-60},"id":"Tl5r93AIdujY","outputId":"9af0fa4e-755b-43c2-96d1-eeeb01396db1"},"outputs":[],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","def transform_data_speaker_disambiguation(file):\n","  string = '<eos>'\n","  space = ' '\n","  inputs = []\n","  for row in file:\n","    if row.find('<eos>') >= 0:\n","      row = row.replace(string, space + string + space)\n","      all_tokens = []\n","      speaker_utterances = row.split('<eos>') #lijst met strings, elke string is een zin van 1 speaker\n","      for utt in speaker_utterances:\n","        tokens = [token.lower() for token in word_tokenize(utt)]\n","        tokens.append('<eos>')\n","        for token in tokens:\n","          all_tokens.append(token)\n","      #tokens = [token.lower() for token in word_tokenize(row)]\n","      #words = row.split()\n","      all_tokens.pop()\n","      inputs.append(all_tokens)\n","\n","  print(\"first utterance in tokens:\", inputs[0])\n","\n","  #speaker disambiguation\n","  disambig_inputs = []\n","  for utterance in inputs:\n","    disambig_utterance = []\n","    turn_counter = 0\n","    for token in utterance:\n","      if token == '<eos>':\n","        turn_counter +=1\n","      disambig_utterance.append(disambiguate_pronouns(token, turn_counter))\n","    disambig_inputs.append(disambig_utterance)\n","\n","  print(\"first utterance in tokens with speaker disambiguation:\", disambig_inputs[0])\n","\n","  return input , disambig_inputs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1671189970263,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"ZAVqUvidP60K","outputId":"72a33576-8a32-4600-91eb-ef90c5227ac7"},"outputs":[],"source":["file = load_evaluation_data(root_dir + '/annotated_data/evaluatiedata/test_declarative_statements_abstract_val_l1.txt')\n","# file = load_evaluation_data(root_dir + '/annotated_data/evaluatiedata/test_coreference_abstract_val_l1.txt')\n","# file = load_evaluation_data(root_dir + '/annotated_data/evaluatiedata/test_single_utterances_abstract_val_l1.txt')\n","input, disambig_inputs = transform_data_speaker_disambiguation(file)\n"]},{"cell_type":"markdown","metadata":{"id":"t2pF0CiRS5gX"},"source":["## level 1 predictions en validatie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gT9SYc0fR72K"},"outputs":[],"source":["tags_count_l1 = len(bio_dict_l1)*2+2\n","tags_count_l1 = list(range(0, tags_count_l1 + 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4544,"status":"ok","timestamp":1671190863100,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"vA_k8BgQ0iJu","outputId":"896b7f9d-7433-4f2b-d29f-e708af1ee355"},"outputs":[],"source":["y_subj, y_pred, y_obj, subwords = model_l1.predict(disambig_inputs[0])\n","\n","# show results\n","for arg, y in [('Subject', y_subj), ('Predicate', y_pred), ('Object', y_obj)]:\n","    print('\\n', arg)\n","    print(''.ljust(15) + '\\t'.join(map(str, tags_count_l1)))\n","    for score, token in zip(y.T, subwords):\n","        score_str = '\\t'.join([\"[\" + str(s)[:5] + \"]\" if s == max(score) else \" \" + str(round(s, 4))[:5] + \" \" for s in score])\n","        token_str = token.replace('▁', '')\n","        print(token_str.ljust(15) + score_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":586,"status":"ok","timestamp":1671190864197,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"El26oCJcE-_F","outputId":"044d91f8-5863-46f7-960d-c8b7527d77ac"},"outputs":[],"source":["for input in disambig_inputs:\n","  y_subj, y_pred, y_obj, subwords = model_l1.predict(input)\n","\n","  print(' '.join(subwords).replace('▁', '') + '\\n')\n","  print('Subjects:  ', bio_tags_to_tokens(subwords, y_subj.T, bio_lookup_l1, one_hot=True))\n","  print('Predicates:', bio_tags_to_tokens(subwords, y_pred.T, bio_lookup_l1, predicate=True, one_hot=True))\n","  print('Objects:   ', bio_tags_to_tokens(subwords, y_obj.T, bio_lookup_l1, one_hot=True))\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"DrlDXBAWTDdW"},"source":["## level 2 predictions en validatie"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":654,"status":"ok","timestamp":1670708714456,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"P9Ms6rgxe_q1","outputId":"8a052fa7-2fc0-4827-e4ad-d1a8cbbe66cd"},"outputs":[],"source":["file = load_evaluation_data(root_dir + '/annotated_data/evaluatiedata/test_declarative_statements_abstract_val_l2.txt')\n","# file = load_evaluation_data(root_dir + '/annotated_data/evaluatiedata/test_coreference_abstract_val_l2.txt')\n","# file = load_evaluation_data(root_dir + '/annotated_data/evaluatiedata/test_single_utterances_abstract_val_l2.txt')\n","input, disambig_inputs = transform_data_speaker_disambiguation(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HsT9N2i5XMUE"},"outputs":[],"source":["tags_count_l2 = len(bio_dict_l2)*2+2\n","tags_count_l2 = list(range(0, tags_count_l2 + 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1188,"status":"ok","timestamp":1670708721318,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"gtKb08e2KE-H","outputId":"74ae215b-ee47-47d6-a126-1b1ed7662698"},"outputs":[],"source":["y_subj, y_pred, y_obj, subwords = model_l2.predict(disambig_inputs[0])\n","\n","# show results\n","for arg, y in [('Subject', y_subj), ('Predicate', y_pred), ('Object', y_obj)]:\n","    print('\\n', arg)\n","    print(''.ljust(15) + '\\t'.join(map(str, tags_count_l2)))\n","    for score, token in zip(y.T, subwords):\n","        score_str = '\\t'.join([\"[\" + str(s)[:5] + \"]\" if s == max(score) else \" \" + str(round(s, 4))[:5] + \" \" for s in score])\n","        token_str = token.replace('▁', '')\n","        print(token_str.ljust(15) + score_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1358,"status":"ok","timestamp":1670709745287,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"7yq1aHlSKLvr","outputId":"5540e31b-100e-41b5-c240-54af42eda3f6"},"outputs":[],"source":["for input in disambig_inputs:\n","  y_subj, y_pred, y_obj, subwords = model_l2.predict(input)\n","\n","  print(' '.join(subwords).replace('▁', '') + '\\n')\n","  print('Subjects:  ', bio_tags_to_tokens(subwords, y_subj.T, bio_lookup_l2, one_hot=True))\n","  print('Predicates:', bio_tags_to_tokens(subwords, y_pred.T, bio_lookup_l2, predicate=True, one_hot=True))\n","  print('Objects:   ', bio_tags_to_tokens(subwords, y_obj.T, bio_lookup_l2, one_hot=True))\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"huUDGyQEJiLe"},"source":["# Ranking the triples\n","\n","Now we are able to extract the candidate arguments, but how do we combine them?\n","\n","We compute all combinations of the subjects, predicates and objects and train a model to distinguish between those triples that are entailed (not considering negation here) and those that are not.\n","\n","For this, we extract a number of negative examples from possible triples, i.e. those combinations of subjects, predicates and objects that were not annotated."]},{"cell_type":"markdown","metadata":{"id":"D_GyAqbPKvFE"},"source":["## Converting format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQDaXHQTJkb9"},"outputs":[],"source":["from collections import defaultdict\n","from copy import deepcopy\n","\n","\n","def extract_triples(annotation, neg_oversampling=7, contr_oversampling=0.7, ellipsis_oversampling=3):\n","    \"\"\" Extracts plain-text triples from an annotation file and samples 'negative' examples by\n","        crossover. By default, the function will over-extract triples with negative polarity and\n","        elliptical constructions to counter class imbalance.\n","\n","        params:\n","        dict annotation:            loaded annotation file (see load_annotations)\n","        int neg_oversampling:       how much to over-sample triples with negative polarity\n","        float contr_oversampling:   how much to sample contrast/invalid triples relative to true triples\n","        int ellipsis_oversampling:  how much to over-sample elliptical triples\n","    \"\"\"\n","    turns = annotation['tokens']\n","    triple_ids = [t[:4] for t in annotation['annotations']]\n","\n","    arguments = defaultdict(list)\n","    triples = []\n","    labels = []\n","\n","    # Oversampling of elliptical triples\n","    for triple in deepcopy(triple_ids):\n","        subj_obj_turns = set([i for i, _ in triple[0] + triple[2]])\n","        if len(subj_obj_turns) > 1:\n","            triple_ids += [triple] * int(ellipsis_oversampling)\n","\n","    # Extract 'True' triples\n","    for subj, pred, obj, polar in triple_ids:\n","\n","        subj = ' '.join(turns[i][j] for i, j in subj) if subj else ''\n","        pred = ' '.join(turns[i][j] for i, j in pred) if pred else ''\n","        obj = ' '.join(turns[i][j] for i, j in obj) if obj else ''\n","\n","        #print(pred)\n","\n","        if subj or pred or obj:\n","\n","            if not polar:\n","                triples += [(subj, pred, obj)]\n","                labels += [1]\n","            else:\n","                triples += [(subj, pred, obj)] * neg_oversampling  # Oversampling negative polarities\n","                labels += [2] * neg_oversampling\n","\n","            arguments['subjs'].append(subj)\n","            arguments['preds'].append(pred)\n","            arguments['objs'].append(obj)\n","\n","    # Skip if the annotation file was blank\n","    if not triples:\n","        return [], [], []\n","\n","    # Sample fake contrast examples (invalid extractions)\n","    n = int(len(triples) * contr_oversampling)\n","    for i in range(50):\n","        s = random.choice(arguments['subjs'])\n","        p = random.choice(arguments['preds'])\n","        o = random.choice(arguments['objs'])\n","\n","        # Ensure samples are new (and not actually valid!)\n","        if (s, p, o) not in triples and s and p and o:\n","            triples += [(s, p, o)]\n","            labels += [0]\n","            n -= 1\n","\n","        # Create as many fake examples as there were 'real' triples\n","        if n == 0:\n","            break\n","\n","    return turns, triples, labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":812,"status":"ok","timestamp":1670924508103,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"PelPIZlwKVpY","outputId":"b79f9f75-b6d3-4c40-c771-39efaa175ee4"},"outputs":[],"source":["tokens, triples, labels = [], [], []\n","for ann in annotations:\n","    ann_tokens, ann_triples, triple_labels = extract_triples(ann)\n","    triples.append(ann_triples)\n","    labels.append(triple_labels)\n","    tokens.append([t for ts in ann_tokens for t in ts + ['<eos>']])\n","\n","j = random.choice(range(len(tokens)))\n","print('tokens: ', tokens[j])\n","print('triples:', triples[j])\n","print('labels: ', labels[j])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3807,"status":"ok","timestamp":1670924614863,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"5x0xwq67nGjh","outputId":"c3ebdc22-aa72-4661-8f7e-651612515490"},"outputs":[],"source":["def abstract_predicates_triples(triples, abstract_dict, lookup, bio_lookup):\n","  dict_values = []\n","  trip = []\n","  for values in abstract_dict.values():\n","    for value in values:\n","      dict_values.append(value)\n","\n","  for row in triples:\n","    abstract_triples = []\n","    for triple in row:\n","      if triple[1] in dict_values:\n","        key = lookup[triple[1]]\n","        abstract_pred = bio_lookup[key[0]]\n","        y = list(triple)\n","        y[1] = abstract_pred\n","        triple = tuple(y)\n","        # print(triple)\n","        abstract_triples.append(triple)\n","      else:\n","        abstract_triples.append(triple)\n","    trip.append(abstract_triples)\n","  return trip\n","\n","triples_l1 = abstract_predicates_triples(triples, bio_dict_l1, lookup_l1, bio_lookup_l1)\n","triples_l2 = abstract_predicates_triples(triples, bio_dict_l2, lookup_l2, bio_lookup_l2)\n","\n","# print(triples)\n","# print(triples_l1)\n","# print(triples_l2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":565,"status":"ok","timestamp":1670924669848,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"KyTk8d2m9VZz","outputId":"2e026d28-4117-4583-e412-371f3d406526"},"outputs":[],"source":["print('Class (im)balance:')\n","print('not entailed  ', sum([np.sum(np.array(t) == 0) for t in labels]))\n","print('entailed (pos)', sum([np.sum(np.array(t) == 1) for t in labels]))\n","print('entailed (neg)', sum([np.sum(np.array(t) == 2) for t in labels]))"]},{"cell_type":"markdown","metadata":{"id":"KSg00eFtKyyK"},"source":["## Fine-tuning ALBERT for Triple Candidate Scoring"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECqJdf96KyX_"},"outputs":[],"source":["class TripleScoring(torch.nn.Module):\n","    def __init__(self, base_model='albert-base-v2', path=None, max_len=80, sep='<eos>'):\n","        super().__init__()\n","        # Base model\n","        print('loading %s for triple scoring' % base_model)\n","        # Load base model\n","        self._model = AutoModel.from_pretrained(base_model)\n","        self._max_len = max_len\n","        self._base = base_model\n","        self._sep = sep\n","\n","        # Load and extend tokenizer with SPEAKERS\n","        self._tokenizer = AutoTokenizer.from_pretrained(base_model)\n","        self._tokenizer.add_tokens(['SPEAKER1', 'SPEAKER2'], special_tokens=True)\n","        self._model.resize_token_embeddings(len(self._tokenizer))\n","\n","        # SPO candidate scoring head\n","        hidden_size = AutoConfig.from_pretrained(base_model).hidden_size\n","        self._head = torch.nn.Linear(hidden_size, 3)\n","        self._relu = torch.nn.ReLU()\n","        self._softmax = torch.nn.Softmax(dim=-1)\n","\n","        # GPU support\n","        self._device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","        self.to(self._device)\n","\n","        # Load model / tokenizer if pretrained model is given\n","        if path:\n","            print('\\t- Loading pretrained')\n","            model_path = glob.glob(path + '/candidate_scorer_' + base_model)[0]\n","            self.load_state_dict(torch.load(model_path, map_location=self._device))\n","\n","    def forward(self, input_ids, speaker_ids, attn_mask):\n","        \"\"\" Computes the forward pass through the model\n","        \"\"\"\n","        out = self._model(input_ids=input_ids, token_type_ids=speaker_ids, attention_mask=attn_mask)\n","        h = self._relu(out.last_hidden_state[:, 0])\n","        return self._softmax(self._head(h))\n","\n","    def _retokenize_dialogue(self, tokens, speaker=1):\n","        # Tokenize each token individually (keeping track of subwords)\n","        f_input_ids = [self._tokenizer.cls_token_id]\n","        speaker_ids = [speaker]\n","        for turn in ' '.join(tokens).split(self._sep):\n","            token_ids = self._tokenizer.encode(turn, add_special_tokens=True)[1:]  # strip [CLS]\n","            f_input_ids += token_ids\n","            speaker_ids += [speaker] * len(token_ids)\n","            speaker = 1 - speaker\n","\n","        return f_input_ids, speaker_ids\n","\n","    def _retokenize_triple(self, triple):\n","        # Append triple\n","        f_input_ids = self._tokenizer.encode(' '.join(triple), add_special_tokens=False)\n","        speaker_ids = [0] * len(f_input_ids)\n","        return f_input_ids, speaker_ids\n","\n","    def _add_padding(self, sequence, pad_token):\n","        # If sequence is too long, cut off end\n","        sequence = sequence[:self._max_len]\n","\n","        # Pad remainder to max_len\n","        padding = self._max_len - len(sequence)\n","        new_sequence = sequence + [pad_token] * padding\n","\n","        # Mask out [PAD] tokens\n","        attn_mask = [1] * len(sequence) + [0] * padding\n","        return new_sequence, attn_mask\n","\n","    def fit(self, tokens, triples, labels, epochs=2, lr=1e-6):\n","        \"\"\" Fits the model to the annotations\n","        \"\"\"\n","        X = []\n","        for tokens, triple_lst, triple_labels in zip(tokens, triples, labels):\n","\n","            # Tokenize dialogue\n","            dialog_input_ids, dialog_speakers = self._retokenize_dialogue(tokens)\n","\n","            for triple, label in zip(triple_lst, triple_labels):\n","                # Tokenize triple\n","                triple_input_ids, triple_speakers = self._retokenize_triple(triple)\n","\n","                # Concatenate dialogue + [UNK] + triple\n","                input_ids = dialog_input_ids[:-1] + [self._tokenizer.unk_token_id] + triple_input_ids\n","                speakers = dialog_speakers[:-1] + [0] + triple_speakers\n","\n","                # Pad sequence with [PAD] to max_len\n","                input_ids, _ = self._add_padding(input_ids, self._tokenizer.pad_token_id)\n","                speakers, attn_mask = self._add_padding(speakers, 0)\n","\n","                # Push Tensor to GPU\n","                input_ids = torch.LongTensor([input_ids]).to(self._device)\n","                speakers = torch.LongTensor([speakers]).to(self._device)\n","                attn_mask = torch.FloatTensor([attn_mask]).to(self._device)\n","                label_ids = torch.LongTensor([label]).to(self._device)\n","\n","                X.append((input_ids, speakers, attn_mask, label_ids))\n","\n","        # Set up optimizer and objective\n","        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n","        criterion = torch.nn.CrossEntropyLoss()\n","\n","        for epoch in range(epochs):\n","            random.shuffle(X)\n","\n","            losses = []\n","            for input_ids, speaker_ids, attn_mask, y in tqdm(X):\n","                # Was the triple entailed? Positively? Negatively?\n","                y_hat = self(input_ids, speaker_ids, attn_mask)\n","                loss = criterion(y_hat, y)\n","                losses.append(loss.item())\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","            print(\"mean loss =\", np.mean(losses))\n","\n","        # Save model to file\n","        torch.save(self.state_dict(), 'candidate_scorer_%s' % self._base)\n","\n","    def predict(self, tokens, triples):\n","        # Tokenize dialogue\n","        dialog_input_ids, dialog_speakers = self._retokenize_dialogue(tokens)\n","\n","        batch_input_ids = []\n","        batch_speakers = []\n","        batch_attn_mask = []\n","\n","        for triple in triples:\n","            # Tokenize triple\n","            triple_input_ids, triple_speakers = self._retokenize_triple(triple)\n","\n","            # Concatenate dialogue + [UNK] + triple\n","            input_ids = dialog_input_ids + [self._tokenizer.unk_token_id] + triple_input_ids\n","            speakers = dialog_speakers + [0] + triple_speakers\n","\n","            # Pad sequence with [PAD] to max_len\n","            input_ids, _ = self._add_padding(input_ids, self._tokenizer.pad_token_id)\n","            speakers, attn_mask = self._add_padding(speakers, 0)\n","\n","            batch_input_ids.append(input_ids)\n","            batch_speakers.append(speakers)\n","            batch_attn_mask.append(attn_mask)\n","\n","        # Push batches to GPU\n","        batch_input_ids = torch.LongTensor(batch_input_ids).to(self._device)\n","        batch_speakers = torch.LongTensor(batch_speakers).to(self._device)\n","        batch_attn_mask = torch.FloatTensor(batch_attn_mask).to(self._device)\n","\n","        label = self(batch_input_ids, batch_speakers, batch_attn_mask)\n","        label = label.cpu().detach().numpy()\n","        return label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJmHFbc00Poe"},"outputs":[],"source":["def save_triple_scorer(level):\n","  import os, shutil\n","\n","  out_dir = root_dir + '/models/candidate_scorer_' + level + '_' + str(date.today())\n","  if not os.path.exists(out_dir):\n","      os.mkdir(out_dir)\n","\n","  shutil.copy('candidate_scorer_albert-base-v2', out_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":36832,"status":"error","timestamp":1670929157623,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"5XKejimSO56J","outputId":"1c738be4-2c84-4152-d533-3a5f611a76f1"},"outputs":[],"source":["scorer_l1 = TripleScoring()\n","scorer_l1.fit(tokens, triples_l1, labels, epochs=7)\n","save_triple_scorer('l1')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":851448,"status":"ok","timestamp":1670933237697,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"LkD5HPLuz8TE","outputId":"b0257acb-efe0-407e-ea6f-d60a923d28c3"},"outputs":[],"source":["scorer_l2 = TripleScoring()\n","scorer_l2.fit(tokens, triples_l2, labels, epochs=7)\n","save_triple_scorer('l2')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":643,"status":"ok","timestamp":1670929204697,"user":{"displayName":"Anniek Jansen","userId":"04730746050473041696"},"user_tz":-60},"id":"38y3V1onaRNn","outputId":"a25bdfdb-a6d2-46de-88f3-745d6215b2d6"},"outputs":[],"source":["# inputs = 'staying here is fine though . SPEAKER1\\'s two dogs keep me company <eos> SPEAKER2 do not love them ! What car do SPEAKER1 drive ? <eos> a toyota . but SPEAKER1 like nissans . <eos>'.split()\n","# triple_examples = [['SPEAKER1', 'drive', 'nissans'],\n","#                    ['SPEAKER1', 'like', 'nissans'], \n","#                    ['SPEAKER2', 'like', 'nissans'], \n","#                    ['SPEAKER2', 'love', 'two dogs'], \n","#                    ['SPEAKER1', 'drive', 'a toyota']]\n","\n","# inputs = '<eos> Do SPEAKER1 work in Amsterdam ? <eos> No , in London . <eos>'.split()\n","# triple_examples = [['SPEAKER1', 'work in', 'Amsterdam']]\n","\n","inputs = 'SPEAKER1 adore unicorns but not photography <eos> What do SPEAKER1 like ? <eos> dogs and gaming, but not cats or elephants . <eos>'.split()\n","triple_examples = [['SPEAKER1', 'adore', 'unicorns'],\n","                   ['SPEAKER1', 'like', 'dogs'],\n","                   ['SPEAKER1', 'like', 'gaming'],\n","                   ['SPEAKER1', 'adore', 'photography'],\n","                   ['SPEAKER1', 'like', 'cats'],\n","                   ['SPEAKER1', 'like', 'elephants'],\n","                   ['SPEAKER1', 'adore', 'elephants'],\n","                   ['SPEAKER1', 'like', 'photography'],\n","                   ['SPEAKER1', 'like', 'unicorns']]\n","\n","np.round(scorer_l1.predict(inputs, triple_examples), 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EnT_NFw0KLE"},"outputs":[],"source":["np.round(scorer_l2.predict(inputs, triple_examples), 3)"]},{"cell_type":"markdown","metadata":{"id":"3rJfyVwGOlBA"},"source":["# Test Argument Extraction on EVAL data"]},{"cell_type":"markdown","metadata":{"id":"sl1VSiioOuzj"},"source":["### Level 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5836,"status":"ok","timestamp":1671285174222,"user":{"displayName":"Dorien Renting","userId":"02236066021659360815"},"user_tz":-60},"id":"W8g5T1JoOrGS","outputId":"7f66cf36-86c6-4b89-af87-7cbab11776a6"},"outputs":[],"source":["#laad getrainde modellen\n","argex_model_l1 = ArgumentExtraction(path=root_dir+'/models/l1_6_40_2022-12-10')\n","scorer_model_l1 = TripleScoring(path=root_dir+'/models/candidate_scorer_l1_2022_12_13')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":627,"status":"ok","timestamp":1671285436379,"user":{"displayName":"Dorien Renting","userId":"02236066021659360815"},"user_tz":-60},"id":"JGCGCXMwQQh-","outputId":"e27d1bdf-b58f-49dd-b430-1ed8b16a7440"},"outputs":[],"source":["#laad eval data en pre-process\n","eval_file = load_evaluation_data(root_dir + '/annotated_data/evaluatiedata/test_declarative_statements_level1_eval.txt')\n","input, disambig_inputs = transform_data_speaker_disambiguation(eval_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1YIglAHjoaqVrZDUUeZ3eYTSPAzipnYDs"},"id":"8Fw0nt96QCsK","outputId":"bc448915-a620-4ef9-9394-4ad0f7218ffc"},"outputs":[],"source":["#extract arguments\n","tags_count_l1 = len(bio_dict_l1)*2+2\n","tags_count_l1 = list(range(0, tags_count_l1 + 1))\n","\n","with open(root_dir+\"/output-l1-single_utt.csv\", 'w') as outf:\n","\n","  for input in disambig_inputs:\n","    y_subj, y_pred, y_obj, subwords = argex_model_l1.predict(input)\n","\n","    \n","    print('\\n')\n","    print(''.ljust(15) + '\\t'.join(map(str, tags_count_l1)))\n","    for score, token in zip(y_pred.T, subwords):\n","        score_str = '\\t'.join([\"[\" + str(s)[:5] + \"]\" if s == max(score) else \" \" + str(round(s, 4))[:5] + \" \" for s in score])\n","        token_str = token.replace('▁', '')\n","        print(token_str.ljust(15) + score_str)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["UJK-Qx8Wq8fX","q-UgD33NqdxC"],"name":"","provenance":[{"file_id":"1JMLbCzzdtQYYrOHQg-j38ulakEXWeUHN","timestamp":1664271069285},{"file_id":"1JdIEIFqyf6bn2hPRfV4diEE1X4YLfeh0","timestamp":1646750393565}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
